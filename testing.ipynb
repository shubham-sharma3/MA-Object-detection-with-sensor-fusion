{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import transforms3d as tf3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import csv\n",
    "import cv2\n",
    "from pyntcloud import PyntCloud as pt\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perspective projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = 6.960217e+02\n",
    "# cy = 2.241806e+02\n",
    "cy = 275\n",
    "# cx = 696\n",
    "fx = 9.597910e+02*1.5\n",
    "fy = 9.599251e+02*1.5\n",
    "# fx = 960\n",
    "# fy = 960\n",
    "K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "dist_coeff = np.array([-3.691481e-01, 1.968681e-01, 1.353473e-03, 5.677587e-04,-6.770705e-02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "        data['SimTime'] = np.round(data['SimTime'], 2)\n",
    "        data.drop('WallTime', inplace=True, axis=1)\n",
    "        data.drop('PropertyType', inplace=True, axis=1)\n",
    "        data.drop('InstanceType', inplace=True, axis=1)\n",
    "        data.drop('ModelInstanceIdPath', inplace=True, axis=1)\n",
    "        index_to_keep = data[data['SimTime'] >= 6.0].index.min()\n",
    "\n",
    "        # Drop rows before the index_to_keep\n",
    "        if index_to_keep is not None:\n",
    "            data = data.iloc[index_to_keep:]\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(gt_path):\n",
    "    data = pd.DataFrame()\n",
    "    gt_data_folders = ['gt100.csv','gt101.csv','gt102.csv','gt103.csv','gt104.csv','gt105.csv','gt106.csv','gt107.csv','gt108.csv','gt109.csv','gt110.csv','gt111.csv','gt112.csv','gt113.csv','gt114.csv','gt115.csv','gt116.csv','gt117.csv','gt118.csv','gt119.csv','gt120.csv']\n",
    "    for file_name in gt_data_folders:\n",
    "        gt_filepath = os.path.join(gt_path, file_name)\n",
    "        gt_data = pd.read_csv(gt_filepath)\n",
    "        gt_data =clean_data(gt_data)\n",
    "        data = pd.concat([data, gt_data], ignore_index=True)\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(\"D:\\\\Thesis\\\\test\\\\color_no_mag_90\\\\color50500.png\")\n",
    "pc_read = o3d.io.read_point_cloud(\"D:\\\\master_thesis\\\\pc_1\\\\pcd100\\\\output.pcd_6000_60.pcd\")\n",
    "# pcd_data = np.asarray(pc_read.points)\n",
    "\n",
    "# data = pd.read_csv(\"D:\\\\\\master_thesis\\\\gt_data\\\\gt101.csv\")\n",
    "# # data_tf = pd.read_csv(\"D:\\\\Thesis\\\\test\\\\tf_90_104.csv\")\n",
    "# data['SimTime'] = np.round(data['SimTime'], 2)\n",
    "# data.drop('WallTime', inplace=True, axis=1)\n",
    "# data.drop('PropertyType', inplace=True, axis=1)\n",
    "# data.drop('InstanceType', inplace=True, axis=1)\n",
    "# data.drop('ModelInstanceIdPath', inplace=True, axis=1)\n",
    "# index_to_keep = data[data['SimTime'] >= 6.0].index.min()\n",
    "\n",
    "# # Drop rows before the index_to_keep\n",
    "# if index_to_keep is not None:\n",
    "#     data = data.iloc[index_to_keep:]\n",
    "    # data_tf = data_tf.iloc[index_to_keep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_path = \"D:\\\\master_thesis\\\\gt_data\"\n",
    "# data = read_data(gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_array_from_column(df, column_name):\n",
    "    vectors = df[column_name].apply(eval)\n",
    "    vectors_df = pd.DataFrame(vectors.tolist())\n",
    "   \n",
    "\n",
    "    return np.asarray(vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "childsizes_all = data.loc[data['PropertyName']=='childsizes']\n",
    "childsizes_array = data_array_from_column(childsizes_all,'Data')\n",
    "childpositions_all = data.loc[data['PropertyName']=='childpositions']\n",
    "childpositions_array = data_array_from_column(childpositions_all,'Data')\n",
    "childorientations_all = data.loc[data['PropertyName']=='childorientations']\n",
    "childorientations_array = data_array_from_column(childorientations_all,'Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "childsizes_filtered = []\n",
    "childpositions_filtered = []\n",
    "childorientations_filtered = []\n",
    "for i in range(childsizes_array.shape[0]):\n",
    "        childsizes_array_temp = [x for x in childsizes_array[i] if x is not None]\n",
    "        childpositions_array_temp = [x for x in childpositions_array[i] if x is not None]   \n",
    "        childorientations_array_temp = [x for x in childorientations_array[i] if x is not None]\n",
    "        childpositions_filtered.append(childpositions_array_temp)\n",
    "        childsizes_filtered.append(childsizes_array_temp) \n",
    "        childorientations_filtered.append(childorientations_array_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_center = []\n",
    "BB_orientation = []\n",
    "BB_length = []\n",
    "BB_width = []\n",
    "BB_height = []\n",
    "# BB_dimensions = []\n",
    "\n",
    "for i in range(len(childsizes_filtered)):\n",
    "    for j in range(len(childsizes_filtered[i])):\n",
    "        length = (childsizes_filtered[i][j][0])*0.8\n",
    "        width = (childsizes_filtered[i][j][1])*0.67\n",
    "        height = (childsizes_filtered[i][j][2])*0.5\n",
    "        # if length <1 and width <1 and height <1:\n",
    "        #     length, width, height = 5.0,2.2,2.4\n",
    "        # else:\n",
    "        #     length, width, height = length, width, height\n",
    "\n",
    "        center = childpositions_filtered[i][j]\n",
    "        orientation = childorientations_filtered[i][j][0]\n",
    "        \n",
    "        BB_length.append(length)\n",
    "        BB_width.append(width)\n",
    "        BB_height.append(height)\n",
    "        # BB_dimensions.append([length, width, height])\n",
    "        BB_center.append(center)\n",
    "        BB_orientation.append(orientation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_objects = len(BB_center)\n",
    "total_frames = int(len(childsizes_array)) # 6 is the number of times childsize is calculated in one time frame\n",
    "total_unique_objects_per_frame = int(total_objects/total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9216, 576, 16)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_objects, total_frames, total_unique_objects_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_center = np.reshape(BB_center, (total_frames, total_unique_objects_per_frame, 3))\n",
    "BB_length = np.reshape(BB_length, (total_frames, total_unique_objects_per_frame))\n",
    "BB_width = np.reshape(BB_width, (total_frames, total_unique_objects_per_frame))\n",
    "BB_height = np.reshape(BB_height, (total_frames, total_unique_objects_per_frame))\n",
    "BB_orientation = np.reshape(BB_orientation, (total_frames, total_unique_objects_per_frame))\n",
    "# BB_dimensions = np.reshape(BB_dimensions, (total_frames, total_unique_objects_per_frame, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_camera_data = data.loc[data['InstanceName']=='Simulated Front Camera (internal reference frame)']\n",
    "tf_camera_data = data.loc[data['InstanceName']=='FrontCamera CCS']\n",
    "tf_camera_array = data_array_from_column(tf_camera_data,'Data').reshape(-1,4,4)\n",
    "\n",
    "tf_lidar_data = data.loc[data['InstanceName']=='Velodyne HDL-64E']\n",
    "tf_lidar_array = data_array_from_column(tf_lidar_data,'Data').reshape(-1,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "camera_projection_array = []\n",
    "# Define camera configuration\n",
    "fov = 51.6  # Field of View (degrees)\n",
    "aspect_ratio = 1392/512  # Widescreen aspect ratio\n",
    "near_plane = 0.01  # Near plane distance\n",
    "far_plane = 500  # Far plane distance\n",
    "camera_position = (0, 2, 5)  # Camera position in world space\n",
    "\n",
    "# Function to calculate tangent of half FOV (common for perspective projection matrix)\n",
    "def tan_half_fov(fov):\n",
    "  radians = math.radians(fov / 2)\n",
    "  return math.tan( radians)\n",
    "\n",
    "# Create an empty perspective projection matrix (replace with library function in practice)\n",
    "camera_projection_matrix = [[0 for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "# Fill upper left 3x3 sub-matrix (common perspective projection structure)\n",
    "half_fov_tan = tan_half_fov(fov)\n",
    "camera_projection_matrix[0][0] = 1 / (aspect_ratio * half_fov_tan)\n",
    "camera_projection_matrix[1][1] = 1 / half_fov_tan\n",
    "camera_projection_matrix[2][2] = (near_plane + far_plane) / (near_plane - far_plane)\n",
    "\n",
    "# Fill other elements based on perspective projection formula (replace with library function in practice)\n",
    "camera_projection_matrix[2][3] = 2 * near_plane * far_plane / (near_plane - far_plane)\n",
    "camera_projection_matrix[3][2] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2C_array = []\n",
    "inv_lidar_tf_array = []\n",
    "for i in range(len(tf_camera_array)):\n",
    "    inv_lidar_tf = np.linalg.inv(tf_lidar_array[i])\n",
    "    V2C = np.dot(inv_lidar_tf,tf_camera_array[i])\n",
    "    flip_tf = np.array([[1,0,0,0],[0,-1,0,0],[0,0,-1,0],[0,0,0,1]])\n",
    "    V2C = np.dot(flip_tf,V2C)\n",
    "\n",
    "    V2C_array.append(V2C)\n",
    "    inv_lidar_tf_array.append(inv_lidar_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:00<00:00, 4027.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# position_from_camera = []\n",
    "position_from_lidar = []\n",
    "for i in tqdm(range(total_frames)):\n",
    "    for j in range(total_unique_objects_per_frame):\n",
    "        world_pts = np.reshape(BB_center[i][j][:3],(3,1))\n",
    "        Rot_world_lidar = inv_lidar_tf_array[i][:3,:3]\n",
    "        trans_world_lidar = np.reshape(inv_lidar_tf_array[i][:3,3],(3,1))\n",
    "        rotated_lidar = np.dot(Rot_world_lidar,world_pts)\n",
    "        pos_from_lidar = rotated_lidar + trans_world_lidar\n",
    "        # dist_lidar_camera = V2C_array[i][:3,3]\n",
    "        # pos_from_camera = [pos_from_lidar[0]-dist_lidar_camera[0], pos_from_lidar[1]-dist_lidar_camera[1], pos_from_lidar[2]-dist_lidar_camera[2]]\n",
    "        # Rot_lidar_camera = V2C_array[i][:3,:3]\n",
    "        # trans_lidar_camera = np.reshape(V2C_array[i][:3,3],(3,1))\n",
    "        # rotated_camera = np.dot(Rot_lidar_camera,pos_from_lidar)\n",
    "        # pos_from_camera = rotated_camera+trans_lidar_camera\n",
    "        position_from_lidar.append(pos_from_lidar)\n",
    "        # position_from_camera.append(pos_from_camera)\n",
    "        \n",
    "    \n",
    "# position_from_camera = np.reshape(position_from_camera, (total_frames, total_unique_objects_per_frame, 3))    \n",
    "position_from_lidar = np.reshape(position_from_lidar, (total_frames, total_unique_objects_per_frame, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_from_camera = []\n",
    "Rt = []\n",
    "# posittion_from_lidar2 = []\n",
    "for i in range(total_frames):\n",
    "    for j in range(total_unique_objects_per_frame):\n",
    "        positionB = np.reshape(BB_center[i][j][:3],(3,1))\n",
    "        TF_CB = np.linalg.inv(tf_camera_array[i].reshape(4,4))\n",
    "        RotCB = TF_CB[:3,:3]\n",
    "        translationCB = np.reshape(TF_CB[:3,3],(3,1)) + np.reshape([0,0.5,1.0],(3,1))\n",
    "        R_T = np.hstack((RotCB,translationCB))\n",
    "        rot = np.dot(RotCB,positionB)\n",
    "        pos = rot + translationCB\n",
    "        # pos_homogeneous = np.vstack((pos,1))\n",
    "        # C2V = np.linalg.inv(V2C_array[i])\n",
    "        # pos_lidar_homogeneous = np.dot(C2V,pos_homogeneous)\n",
    "        # pos_lidar = pos_lidar_homogeneous[:3]\n",
    "        position_from_camera.append(pos)\n",
    "        Rt.append(R_T)\n",
    "        # posittion_from_lidar2.append(pos_lidar)\n",
    "    \n",
    "    \n",
    "position_from_camera = np.reshape(position_from_camera, (total_frames, total_unique_objects_per_frame, 3))\n",
    "Rt = np.reshape(Rt,(total_frames,total_unique_objects_per_frame,3,4))\n",
    "# posittion_from_lidar2 = np.reshape(posittion_from_lidar2, (total_frames, total_unique_objects_per_frame, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V = tf_lidar_array[445]\n",
    "W2C = tf_camera_array[445]\n",
    "V2W = np.linalg.inv(W2V)\n",
    "flip_tf = np.array([[1,0,0,0],[0,-1,0,0],[0,0,-1,0],[0,0,0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "V2C = flip_tf@V2W@W2C\n",
    "C2V = np.linalg.inv(V2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = V2C_array[445]\n",
    "cv = np.linalg.inv(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-9.17417916e-03,  2.41397757e+01, -9.11517660e-01]),\n",
       " array([-9.56326496e-03,  8.07518216e-01,  2.39536817e+01]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2V[:3,:3]@position_from_camera[445][7],V2C[:3,:3]@position_from_lidar[445][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.44480859e-03,  2.37437757e+01, -1.72542397e+00])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2V[:3,:3]@position_from_camera[445][7]+C2V[:3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-9.58722450e-03,  2.39536817e+01, -8.07517932e-01]),\n",
       " array([-9.14713388e-03,  9.11517932e-01,  2.41397757e+01]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_from_lidar[445][7] , position_from_camera[445][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_y = []\n",
    "for i in range((total_frames)):\n",
    "    for j in range((total_unique_objects_per_frame)):\n",
    "        yaw_vehicle = np.rad2deg(BB_orientation[i][j])\n",
    "        r = R.from_matrix(tf_camera_array[i][:3,:3])\n",
    "        roll, pitch, yaw = (r.as_euler('xyz', degrees=True))\n",
    "        angle_diff = np.deg2rad((yaw+90.0)-yaw_vehicle)\n",
    "        rotation_y.append(angle_diff)\n",
    "rotation_y = np.reshape(rotation_y, (total_frames, total_unique_objects_per_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BB_lidar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bb_index = []\n",
    "\n",
    "Lidar_clipping_range = [5, -20, -3, 40, 20, 1]\n",
    "for i in range(total_frames):\n",
    "    temp_list = []\n",
    "    for idx, j in enumerate(position_from_lidar[i]):\n",
    "        if j[0] < Lidar_clipping_range[4] and j[0] > Lidar_clipping_range[1] and j[1] < Lidar_clipping_range[3] and j[1] > Lidar_clipping_range[0] and j[2] < Lidar_clipping_range[5] and j[2] > Lidar_clipping_range[2]:\n",
    "            temp_list.append(idx)\n",
    "    filtered_bb_index.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BB_coordinates_lidar = []\n",
    "# for i in range((total_frames)):\n",
    "#   for j in range((total_unique_objects_per_frame)):\n",
    "#         x = position_from_lidar[i][j][0]\n",
    "#         y = position_from_lidar[i][j][1]\n",
    "#         z = position_from_lidar[i][j][2]\n",
    "\n",
    "#         l = BB_length[i][j]\n",
    "#         h = BB_height[i][j]\n",
    "#         w = BB_width[i][j]\n",
    "\n",
    "#         theta = rotation_y[i][j]\n",
    "#         Rot_matrix_bb = tf3d.euler.euler2mat(0,0,theta)\n",
    "\n",
    "#         edges = []\n",
    "#         oriented_edges = []\n",
    "#         e1 = (x-w,y-l,z-h)\n",
    "#         e2 = (x-w,y-l,z+h)\n",
    "#         e3 = (x-w,y+l,z+h)\n",
    "#         e4 = (x-w,y+l,z-h)\n",
    "#         e5 = (x+w,y-l,z-h)\n",
    "#         e6 = (x+w,y-l,z+h)\n",
    "#         e7 = (x+w,y+l,z+h)\n",
    "#         e8 = (x+w,y+l,z-h)\n",
    "#         edges.append(e1)\n",
    "#         edges.append(e2)\n",
    "#         edges.append(e3)\n",
    "#         edges.append(e4)\n",
    "#         edges.append(e5)\n",
    "#         edges.append(e6)\n",
    "#         edges.append(e7)\n",
    "#         edges.append(e8)\n",
    "#         # BB_coordinates_lidar.append(edges)\n",
    "#         for edge in edges:\n",
    "#             edge = np.dot(Rot_matrix_bb, edge)\n",
    "#             BB_coordinates_lidar.append(edge)\n",
    "\n",
    "# BB_coordinates_lidar = np.reshape( BB_coordinates_lidar,(total_frames,total_unique_objects_per_frame,8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BB_camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_coordinates_camera = []\n",
    "# BB_coordinates_lidar = []\n",
    "for i in range((total_frames)):\n",
    "  for j in range((total_unique_objects_per_frame)):\n",
    "        l, h, w = BB_length[i][j], BB_height[i][j], BB_width[i][j]\n",
    "        x_corners = [w / 2, w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2]\n",
    "        # y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n",
    "        y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2]\n",
    "        z_corners = [l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2]\n",
    "\n",
    "        R = np.array([[np.cos(rotation_y[i][j]), 0, np.sin(rotation_y[i][j])],\n",
    "                        [0, 1, 0],\n",
    "                        [-np.sin(rotation_y[i][j]), 0, np.cos(rotation_y[i][j])]])\n",
    "        corners3d = np.vstack([x_corners, y_corners, z_corners])  # (3, 8)\n",
    "        corners3d = np.dot(R, corners3d).T\n",
    "        corners3d = corners3d + position_from_camera[i][j]\n",
    "        C2V = np.linalg.inv(V2C_array[i])\n",
    "        corners3d_lidar = np.dot(C2V[:3, :3], corners3d.T) + C2V[:3, 3].reshape(-1, 1)\n",
    "        BB_coordinates_camera.append(corners3d)\n",
    "        \n",
    "BB_coordinates_camera = np.reshape( BB_coordinates_camera,(total_frames,total_unique_objects_per_frame,8,3))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BB coordinates LIDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BB_coordinates_lidar = []\n",
    "# for i in range((total_frames)):\n",
    "#   for j in range((total_unique_objects_per_frame)):\n",
    "#     l, h, w = (BB_length[i][j])/0.67, (BB_height[i][j])/0.67, (BB_width[i][j])/0.67\n",
    "#     x_corners = [w / 2, w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2]\n",
    "#     # y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n",
    "#     y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2]\n",
    "#     z_corners = [l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2]\n",
    "\n",
    "#     R = np.array([[np.cos(rotation_y[i][j]), 0, np.sin(rotation_y[i][j])],\n",
    "#                     [0, 1, 0],\n",
    "#                     [-np.sin(rotation_y[i][j]), 0, np.cos(rotation_y[i][j])]])\n",
    "#     corners3d = np.vstack([x_corners, y_corners, z_corners])  # (3, 8)\n",
    "#     corners3d = np.dot(R, corners3d).T\n",
    "#     C2V = np.linalg.inv(V2C_array[i])\n",
    "#     position_camera_homogeneous = np.array([position_from_camera[i][j][0], position_from_camera[i][j][1], position_from_camera[i][j][2], 1])\n",
    "#     position_lidar_homogeneous = np.dot(C2V, position_camera_homogeneous)\n",
    "#     position_from_lidar = position_lidar_homogeneous[:3]\n",
    "#     corners3d_lidar = corners3d + position_from_lidar\n",
    "        \n",
    "        \n",
    "#     BB_coordinates_lidar.append(corners3d_lidar)\n",
    "# BB_coordinates_lidar = np.reshape( BB_coordinates_lidar,(total_frames,total_unique_objects_per_frame,8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fov calcu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.60182925042597"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 4.5*10-3\n",
    "w = p*1392\n",
    "f = fx*p\n",
    "fov = np.rad2deg(2*np.arctan(w/(2*f)))\n",
    "fov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMERA BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mundistort(\u001b[43mimage\u001b[49m, K, dist_coeff)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "image = cv2.undistort(image, K, dist_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_bb_corners = []\n",
    "# for i in range(total_frames):\n",
    "#     for j in range(total_unique_objects_per_frame):\n",
    "#         for k in range(len(BB_coordinates_camera[i][j])):\n",
    "#             translated_point = BB_coordinates_camera[i][j][k]\n",
    "#             homogenous_3d_point = np.array([translated_point[0], translated_point[1], translated_point[2], 1])\n",
    "#             clip_pt = np.dot(camera_projection_matrix, homogenous_3d_point.T)\n",
    "#             normalized_device_coordinates = clip_pt / clip_pt[3]\n",
    "#             screen_width = 1392\n",
    "#             screen_height = 512\n",
    "#             image_x = int((normalized_device_coordinates[0] + 1) * screen_width / 2)\n",
    "#             image_y = int((1 - normalized_device_coordinates[1]) * screen_height / 2)\n",
    "#             # proj_pt = K@translated_point.T\n",
    "#             # point_2d = (proj_pt[:-1] / proj_pt[-1]).astype(int)\n",
    "#             point_2d = (image_x, image_y)\n",
    "#             image_bb_corners.append(point_2d)\n",
    "# image_bb_corners = np.reshape(image_bb_corners,(total_frames,total_unique_objects_per_frame,8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncation(position_from_camera, BB_length, BB_width, BB_height,K, BB_orientation):\n",
    "    truncation = []\n",
    "    for i in range(filtered_bb_index):\n",
    "        temp_list = []\n",
    "        for j in range(filtered_bb_index[i]):\n",
    "            center = position_from_camera[i][j]\n",
    "            l = BB_length[i][j]*0.8\n",
    "            w = BB_width[i][j]*0.67\n",
    "            h = BB_height[i][j]*0.5\n",
    "            if BB_orientation[i][j] >-0.7853 and BB_orientation[i][j] < 0.7853:\n",
    "                car_left_end = [center[0] + w/2, center[1], center[2]] # sideways\n",
    "                car_right_end = [center[0] - w/2, center[1], center[2]]\n",
    "            else:\n",
    "                car_left_end = [center[0] + l/2, center[1], center[2]]\n",
    "                car_right_end = [center[0] - l/2, center[1], center[2]]\n",
    "            car_bottom = [center[0], center[1]-h/2, center[2]] # infront\n",
    "            car_roof = [center[0], center[1]+h/2, center[2]]\n",
    "            # car_right = [center[0] + w/2, center[1], center[2]]\n",
    "            # car_left = [center[0] - w/2, center[1], center[2]]\n",
    "            proj_pt = K@center.T\n",
    "            proj_right_end = K@car_right_end.T\n",
    "            proj_left_end = K@car_left_end.T\n",
    "            proj_roof = K@car_roof.T\n",
    "            proj_bottom = K@car_bottom.T\n",
    "            # proj_right = K@car_right.T\n",
    "            \n",
    "            u,v = (proj_pt[:-1] / proj_pt[-1]).astype(int)\n",
    "            p,_ = (proj_right_end[:-1] / proj_right_end[-1]).astype(int)\n",
    "            q,_ = (proj_left_end[:-1] / proj_left_end[-1]).astype(int)\n",
    "            _,n = (proj_bottom[:-1] / proj_bottom[-1]).astype(int)\n",
    "            _,m = (proj_roof[:-1] / proj_roof[-1]).astype(int)\n",
    "            proj_l = (p-u)*2\n",
    "            # proj_w = (s-u)*2\n",
    "            proj_h = (n -v)*2\n",
    "            if p < 1392 and q > 0  and n < 512:\n",
    "                trunc = 0\n",
    "            elif q >= 1392 or p <= 0 or m>=512 :\n",
    "                trunc = 1\n",
    "            elif q<1392 and p>1392:\n",
    "                trunc = (p - 1392)/proj_l\n",
    "            elif q < 0 and p > 0:\n",
    "                trunc = (0-q)/proj_l\n",
    "            elif n > 512 and m < 512:\n",
    "                trunc = (n-512)/proj_h\n",
    "\n",
    "            temp_list.append(trunc)\n",
    "        truncation.append(temp_list)\n",
    "            \n",
    "    return truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bb_corners = []\n",
    "for i in range(total_frames):\n",
    "    for j in range(total_unique_objects_per_frame):\n",
    "        for k in range(len(BB_coordinates_camera[i][j])):\n",
    "            translated_point = BB_coordinates_camera[i][j][k]\n",
    "            # new_point = np.array([translated_point[0],translated_point[2],translated_point[1]])\n",
    "            # proj_pt = K@new_point.T\n",
    "            proj_pt = K@translated_point.T\n",
    "            point_2d = (proj_pt[:-1] / proj_pt[-1]).astype(int)\n",
    "            image_bb_corners.append(point_2d)\n",
    "image_bb_corners = np.reshape(image_bb_corners,(total_frames,total_unique_objects_per_frame,8,2))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_from_camera[445][8], position_from_camera[445][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BB_coordinates_camera[445][8], BB_coordinates_camera[445][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_bb = image_bb_corners[445][7]\n",
    "test_lidar_bb = BB_coordinates_lidar[445][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = np.max(test_img_bb[:, 0])\n",
    "min_x = np.min(test_img_bb[:, 0])\n",
    "max_y = np.max(test_img_bb[:, 1])\n",
    "min_y = np.min(test_img_bb[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806, 647, 409, 237)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_x, min_x, max_y, min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (255, 255, 255), 2)\n",
    "# cv2.rectangle(image, (min_x, ymin), (max_x, ymax), (0, 0,0), 2)\n",
    "# \n",
    "# Display the image with the 2D bounding box\n",
    "cv2.imshow(\"2D Bounding Box\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(pcd_data)\n",
    "o3d.io.write_point_cloud(\"pcd1.ply\",pcd1)\n",
    "\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd2.points = o3d.utility.Vector3dVector(test_lidar_bb)\n",
    "o3d.io.write_point_cloud(\"pcd2.ply\",pcd2)\n",
    "\n",
    "pcd1_read = o3d.io.read_point_cloud(\"pcd1.ply\")\n",
    "pcd2_read = o3d.io.read_point_cloud(\"pcd2.ply\")\n",
    "\n",
    "pcd1_read.paint_uniform_color([0,0,0.75]) # choose a unique rgb value for each point cloud\n",
    "pcd2_read.paint_uniform_color([0,0.5,0])\n",
    "\n",
    "bb_data = o3d.geometry.AxisAlignedBoundingBox.create_from_points(pcd2.points)\n",
    "obb_data = o3d.geometry.AxisAlignedBoundingBox.get_oriented_bounding_box(bb_data)\n",
    "# o3d.visualization.draw_geometries([pcd1_read,pcd2_read])\n",
    "o3d.visualization.draw_geometries([pcd1_read,obb_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 255, 0)  # Green color for the bounding box (BGR format)\n",
    "thickness = 2  # Thickness of the bounding box lines\n",
    "cv2.line(image, test_img_bb[0], test_img_bb[1], color, thickness)\n",
    "cv2.line(image, test_img_bb[1], test_img_bb[2], color, thickness)\n",
    "cv2.line(image, test_img_bb[2], test_img_bb[3], color, thickness)\n",
    "cv2.line(image, test_img_bb[3], test_img_bb[0], color, thickness)\n",
    "cv2.line(image, test_img_bb[4], test_img_bb[5], color, thickness)\n",
    "cv2.line(image, test_img_bb[5], test_img_bb[6], color, thickness)\n",
    "cv2.line(image, test_img_bb[6], test_img_bb[7], color, thickness)\n",
    "cv2.line(image, test_img_bb[7], test_img_bb[4], color, thickness)\n",
    "cv2.line(image, test_img_bb[0], test_img_bb[4], color, thickness)\n",
    "cv2.line(image, test_img_bb[1], test_img_bb[5], color, thickness)\n",
    "cv2.line(image, test_img_bb[2], test_img_bb[6], color, thickness)\n",
    "cv2.line(image, test_img_bb[3], test_img_bb[7], color, thickness)\n",
    "\n",
    "# Display the image with the bounding box\n",
    "cv2.imshow('Bounding Box', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLpractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
